{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "本WorkShop包含的代码可以在MNIST上训练完全连接的深层神经网络。 以前笔记本的主要变化是：\n",
    "*我们已经从线性分类器切换到深度神经网络。\n",
    "\n",
    "*我们添加了代码来显示TensorBoard中的图形和摘要数据。\n",
    "\n",
    "*我们正在使用AdamOptimizer而不是使用GradientDescentOptimizer。\n",
    "\n",
    "*我们正在使用Dropout。\n",
    "\n",
    "一个重要的要点：注意训练模型的代码与以前的Workshop相同，尽管模型更复杂。\n",
    "\n",
    "通过运行单元或者取消注释代码来尝试本Workshop\n",
    "\n",
    "当你完成这个笔记本，使用TensorBoard来查看结果。\n",
    "\n",
    "虽然这是一个简单的模型，但我们可以在MNIST上实现> 97％的准确度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化，数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输出的位置\n",
    "LOGDIR = './graphs' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data\\train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 获取数据\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隐藏层的神经元数量\n",
    "HIDDEN1_SIZE = 500\n",
    "HIDDEN2_SIZE = 250\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "NUM_PIXELS = 28 * 28\n",
    "\n",
    "#  训练的次数\n",
    "#  每个训练批次的时间\n",
    "TRAIN_STEPS = 2000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 学习速率\n",
    "# notebook, and a new optimizer\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "with tf.name_scope('input'):\n",
    "    images = tf.placeholder(tf.float32, [None, NUM_PIXELS], name=\"pixels\")\n",
    "    labels = tf.placeholder(tf.float32, [None, NUM_CLASSES], name=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a fully connected layer\n",
    "def fc_layer(input, size_out, name=\"fc\", activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        size_in = int(input.shape[1])\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"weights\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"bias\")\n",
    "        wx_plus_b = tf.matmul(input, w) + b\n",
    "        if activation: return activation(wx_plus_b)\n",
    "        return wx_plus_b\n",
    "    \n",
    "# The way we initialize variables has an affect on how quickly \n",
    "# training converges. We may explore with different strategies later.\n",
    "# w = tf.Variable(tf.truncated_normal(shape=[size_in, size_out], stddev=1.0 / math.sqrt(float(size_in))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "\n",
    "# First, we'll create two fully connected layers, with ReLU activations\n",
    "#首先，我们定义俩层全连接层,并使用激励函数\n",
    "fc1 = fc_layer(images, HIDDEN1_SIZE, \"fc1\", activation=tf.nn.relu)\n",
    "fc2 = fc_layer(fc1, HIDDEN2_SIZE, \"fc2\", activation=tf.nn.relu)\n",
    "\n",
    "# Next, we'll apply Dropout to the second layer\n",
    "# This can help prevent overfitting, and I've added it here\n",
    "# for illustration. You can comment this out, if you like.\n",
    "\n",
    "dropped = tf.nn.dropout(fc2, keep_prob=0.9)\n",
    "\n",
    "# Finally, we'll calculate logists. This will be\n",
    "# the input to our Softmax function. Notice we \n",
    "# don't apply an activation at this layer.\n",
    "# If you've commented out the dropout layer,\n",
    "# switch the input here to 'fc2'.\n",
    "y = fc_layer(dropped, NUM_CLASSES, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数与优化器\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y, labels=labels))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    # Whereas in the previous notebook we used a vanilla GradientDescentOptimizer\n",
    "    # here, we're using Adam. This is a single line of code change, and more\n",
    "    # importantly, TensorFlow will still automatically analyze our graph\n",
    "    # and determine how to adjust the variables to decrease the loss.\n",
    "    train = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "with tf.name_scope(\"evaluation\"):\n",
    "    # these there lines are identical to the previous notebook.\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging.\n",
    "# We'll use a second FileWriter to summarize accuracy on\n",
    "# the test set. This will let us display it nicely in TensorBoard.\n",
    "train_writer = tf.summary.FileWriter(os.path.join(LOGDIR, \"train\"))\n",
    "train_writer.add_graph(sess.graph)\n",
    "test_writer = tf.summary.FileWriter(os.path.join(LOGDIR, \"test\"))\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.216100 at step 0\n",
      "test accuracy: 0.924500 at step 100\n",
      "test accuracy: 0.936900 at step 200\n",
      "test accuracy: 0.956500 at step 300\n",
      "test accuracy: 0.955400 at step 400\n",
      "test accuracy: 0.958800 at step 500\n",
      "test accuracy: 0.967100 at step 600\n",
      "test accuracy: 0.964900 at step 700\n",
      "test accuracy: 0.967200 at step 800\n",
      "test accuracy: 0.970600 at step 900\n",
      "test accuracy: 0.970000 at step 1000\n",
      "test accuracy: 0.965500 at step 1100\n",
      "test accuracy: 0.974000 at step 1200\n",
      "test accuracy: 0.972600 at step 1300\n",
      "test accuracy: 0.973100 at step 1400\n",
      "test accuracy: 0.970400 at step 1500\n",
      "test accuracy: 0.972700 at step 1600\n",
      "test accuracy: 0.977600 at step 1700\n",
      "test accuracy: 0.977400 at step 1800\n",
      "test accuracy: 0.972900 at step 1900\n",
      "Accuracy 0.977300\n"
     ]
    }
   ],
   "source": [
    "for step in range(TRAIN_STEPS):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "    summary_result, _ = sess.run([summary_op, train], \n",
    "                                    feed_dict={images: batch_xs, labels: batch_ys})\n",
    "\n",
    "    train_writer.add_summary(summary_result, step)\n",
    "    train_writer.add_run_metadata(tf.RunMetadata(), 'step%03d' % step)\n",
    "    \n",
    "    # calculate accuracy on the test set, every 100 steps.\n",
    "    # we're using the entire test set here, so this will be a bit slow\n",
    "    if step % 100 == 0:\n",
    "        summary_result, acc = sess.run([summary_op, accuracy], \n",
    "                                       feed_dict={images: mnist.test.images, \n",
    "                                                  labels: mnist.test.labels})\n",
    "        test_writer.add_summary(summary_result, step)\n",
    "        test_writer.add_run_metadata(tf.RunMetadata(), 'step%03d' % step)\n",
    "        print (\"test accuracy: %f at step %d\" % (acc, step))\n",
    "\n",
    "\n",
    "print(\"Accuracy %f\" % sess.run(accuracy, \n",
    "                               feed_dict={images: mnist.test.images,\n",
    "                                          labels: mnist.test.labels}))\n",
    "train_writer.close()\n",
    "test_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
